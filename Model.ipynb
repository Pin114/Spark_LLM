{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e4fe02-a841-4897-9c9d-fa6fc1159b37",
   "metadata": {},
   "source": [
    "# Loading in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02486e21-65c4-4024-91b8-8e34c82d8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenpinyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd6112a-65b9-40ab-94fb-d7dec2ae91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your collected CSV\n",
    "data_path = r\"/Users/chenpinyu/Desktop/spark/notebooks/Dirty_structured_output.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44c601c-20b1-4a1e-a703-86810c1048d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>main_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2504.02678v1</td>\n",
       "      <td>Valley and Spin Polarized States in Bernal Bil...</td>\n",
       "      <td>We present the results for the evolution of th...</td>\n",
       "      <td>cond-mat.str-el</td>\n",
       "      <td>cond-mat.str-el,cond-mat.mes-hall</td>\n",
       "      <td>2025-04-03T15:15:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2504.15872v1</td>\n",
       "      <td>Multiscale detection of practically significan...</td>\n",
       "      <td>In many change point problems it is reasonable...</td>\n",
       "      <td>stat.ME</td>\n",
       "      <td>stat.ME,math.ST,stat.TH</td>\n",
       "      <td>2025-04-22T13:16:44Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2504.17570v1</td>\n",
       "      <td>Atemporality from Conservation Laws of Physics...</td>\n",
       "      <td>Recent results have shown that singularities c...</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>gr-qc,astro-ph.HE,hep-th</td>\n",
       "      <td>2025-04-24T13:59:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2504.16897v1</td>\n",
       "      <td>Assessing SSL/TLS Certificate Centralization: ...</td>\n",
       "      <td>SSL/TLS is a fundamental technology in the net...</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>2025-04-23T17:26:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2504.11344v1</td>\n",
       "      <td>Interpretable Hybrid-Rule Temporal Point Proce...</td>\n",
       "      <td>Temporal Point Processes (TPPs) are widely use...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,cs.AI,stat.ML</td>\n",
       "      <td>2025-04-15T16:15:16Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>http://arxiv.org/abs/2504.17321v1</td>\n",
       "      <td>Dargana: fine-tuning EarthPT for dynamic tree ...</td>\n",
       "      <td>We present Dargana, a fine-tuned variant of th...</td>\n",
       "      <td>physics.geo-ph</td>\n",
       "      <td>physics.geo-ph,cs.LG</td>\n",
       "      <td>2025-04-24T07:23:27Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>http://arxiv.org/abs/2504.01347v1</td>\n",
       "      <td>MEEK: Re-thinking Heterogeneous Parallel Error...</td>\n",
       "      <td>Heterogeneous parallel error detection is an a...</td>\n",
       "      <td>cs.AR</td>\n",
       "      <td>cs.AR</td>\n",
       "      <td>2025-04-02T04:32:49Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>http://arxiv.org/abs/2504.02741v1</td>\n",
       "      <td>A Complete Classification of Fourier Summation...</td>\n",
       "      <td>We completely classify Fourier summation formu...</td>\n",
       "      <td>math.CA</td>\n",
       "      <td>math.CA,math.MG,math.NT</td>\n",
       "      <td>2025-04-03T16:28:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>http://arxiv.org/abs/2503.24223v1</td>\n",
       "      <td>Jordanian deformation of the non-compact and $...</td>\n",
       "      <td>Using a Drinfeld twist of Jordanian type, we c...</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,cond-mat.stat-mech,cond-mat.str-el,quan...</td>\n",
       "      <td>2025-03-31T15:38:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>http://arxiv.org/abs/2504.10884v1</td>\n",
       "      <td>Unusual gas sensor response and semiconductor-...</td>\n",
       "      <td>WO3-x thinfilms featuring petal-like and lamel...</td>\n",
       "      <td>cond-mat.mtrl-sci</td>\n",
       "      <td>cond-mat.mtrl-sci,cond-mat.str-el</td>\n",
       "      <td>2025-04-15T05:29:29Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7702 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    aid  \\\n",
       "0     http://arxiv.org/abs/2504.02678v1   \n",
       "1     http://arxiv.org/abs/2504.15872v1   \n",
       "2     http://arxiv.org/abs/2504.17570v1   \n",
       "3     http://arxiv.org/abs/2504.16897v1   \n",
       "4     http://arxiv.org/abs/2504.11344v1   \n",
       "...                                 ...   \n",
       "7697  http://arxiv.org/abs/2504.17321v1   \n",
       "7698  http://arxiv.org/abs/2504.01347v1   \n",
       "7699  http://arxiv.org/abs/2504.02741v1   \n",
       "7700  http://arxiv.org/abs/2503.24223v1   \n",
       "7701  http://arxiv.org/abs/2504.10884v1   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Valley and Spin Polarized States in Bernal Bil...   \n",
       "1     Multiscale detection of practically significan...   \n",
       "2     Atemporality from Conservation Laws of Physics...   \n",
       "3     Assessing SSL/TLS Certificate Centralization: ...   \n",
       "4     Interpretable Hybrid-Rule Temporal Point Proce...   \n",
       "...                                                 ...   \n",
       "7697  Dargana: fine-tuning EarthPT for dynamic tree ...   \n",
       "7698  MEEK: Re-thinking Heterogeneous Parallel Error...   \n",
       "7699  A Complete Classification of Fourier Summation...   \n",
       "7700  Jordanian deformation of the non-compact and $...   \n",
       "7701  Unusual gas sensor response and semiconductor-...   \n",
       "\n",
       "                                                summary      main_category  \\\n",
       "0     We present the results for the evolution of th...    cond-mat.str-el   \n",
       "1     In many change point problems it is reasonable...            stat.ME   \n",
       "2     Recent results have shown that singularities c...              gr-qc   \n",
       "3     SSL/TLS is a fundamental technology in the net...              cs.NI   \n",
       "4     Temporal Point Processes (TPPs) are widely use...              cs.LG   \n",
       "...                                                 ...                ...   \n",
       "7697  We present Dargana, a fine-tuned variant of th...     physics.geo-ph   \n",
       "7698  Heterogeneous parallel error detection is an a...              cs.AR   \n",
       "7699  We completely classify Fourier summation formu...            math.CA   \n",
       "7700  Using a Drinfeld twist of Jordanian type, we c...             hep-th   \n",
       "7701  WO3-x thinfilms featuring petal-like and lamel...  cond-mat.mtrl-sci   \n",
       "\n",
       "                                             categories             published  \n",
       "0                     cond-mat.str-el,cond-mat.mes-hall  2025-04-03T15:15:42Z  \n",
       "1                               stat.ME,math.ST,stat.TH  2025-04-22T13:16:44Z  \n",
       "2                              gr-qc,astro-ph.HE,hep-th  2025-04-24T13:59:42Z  \n",
       "3                                                 cs.NI  2025-04-23T17:26:18Z  \n",
       "4                                   cs.LG,cs.AI,stat.ML  2025-04-15T16:15:16Z  \n",
       "...                                                 ...                   ...  \n",
       "7697                               physics.geo-ph,cs.LG  2025-04-24T07:23:27Z  \n",
       "7698                                              cs.AR  2025-04-02T04:32:49Z  \n",
       "7699                            math.CA,math.MG,math.NT  2025-04-03T16:28:18Z  \n",
       "7700  hep-th,cond-mat.stat-mech,cond-mat.str-el,quan...  2025-03-31T15:38:04Z  \n",
       "7701                  cond-mat.mtrl-sci,cond-mat.str-el  2025-04-15T05:29:29Z  \n",
       "\n",
       "[7702 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa70043-7dc7-4d01-9585-d4e8c0e6067b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fcd4a-5822-4355-8f72-b859f4a2db23",
   "metadata": {},
   "source": [
    "#### Dropping duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e12cbac-5cd5-49c3-b7ae-d14f579fc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "## Drop duplicate rows from the dataset\n",
    "df=df.drop_duplicates()\n",
    "## Checking if some duplicates still remain based on \"aid\"(if the output is empty all duplicates have been removed)\n",
    "value_counts_result = df[\"aid\"].value_counts()\n",
    "filtered_result = value_counts_result[value_counts_result > 1]\n",
    "print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5164e4-209b-43de-b480-8dea5f46c555",
   "metadata": {},
   "source": [
    "#### Dropping rows with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b21516-4298-44c6-b600-d2a90c3de3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notnull() & df['summary'].notnull()]\n",
    "df = df[df['categories'].notnull()]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dae779-81ee-4c00-aaea-711a1ea57418",
   "metadata": {},
   "source": [
    "#### Removing subcategories after \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd01a1fe-baf2-4b17-84b1-0bc154d8c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_category'] = df['main_category'].apply(lambda x: x.split('.')[0] if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343db9d7-0b0b-4c17-bbfa-1df3e69769e6",
   "metadata": {},
   "source": [
    "#### Combining Title and Summary into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc127dd-57cb-4565-8ec0-011d0a36d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'] + ' ' + df['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ca81-f613-43c7-8ae6-2a891dd36a26",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd84abc-d4bb-4a9f-af98-c86a6d5b7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['text', 'main_category']], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db64083-7978-42a8-bcb7-4ec148f95eef",
   "metadata": {},
   "source": [
    "#### Convert to spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc5362-d499-40a1-849e-4d23e8ceae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 11:13:40 WARN Utils: Your hostname, Pins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.46.219.58 instead (on interface en0)\n",
      "25/05/01 11:13:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/01 11:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CategoryPrediction\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"300s\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate() #attempt by chatgpt to fix error i get in second cell below this\n",
    "\n",
    "train_df = spark.createDataFrame(train)\n",
    "test_df = spark.createDataFrame(test)\n",
    "train_df = train_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbbe3926-9d88-48e4-884c-26d15d35d078",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o901.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 9.0 failed 1 times, most recent failure: Lost task 13.0 in stage 9.0 (TID 78) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:576)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:539)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:576)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:539)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 37 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# this already gives an error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\spark\\.pixi\\envs\\default\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o901.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 9.0 failed 1 times, most recent failure: Lost task 13.0 in stage 9.0 (TID 78) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:576)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:539)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 37 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:474)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:576)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:539)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 37 more\r\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5) # this already gives an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa54f5a-2eff-4b97-8abb-4551bc0d29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a3b68e-6145-4a58-bb47-913f0ca23160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "preprocessor = preprocessing_pipeline.fit(train_df)\n",
    "\n",
    "train_preprocessed = preprocessor.transform(train_df)\n",
    "test_preprocessed = preprocessor.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d785bcd-3844-4e8c-82c9-946b1392acba",
   "metadata": {},
   "source": [
    "#### This was just to test whether tokenizer for example even works for a very small data example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4906a-e851-4c20-ae5c-b7dba80743cd",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381c966-be89-483d-821e-0629fb40dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 假設 train_preprocessed 和 test_preprocessed 是你的 Spark DataFrames\n",
    "spark = SparkSession.builder.appName(\"CategoryPrediction\").getOrCreate()\n",
    "train_pd = train_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "test_pd = test_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "\n",
    "# 編碼 labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_pd['label'] = label_encoder.fit_transform(train_pd['main_category'])\n",
    "test_pd['label'] = label_encoder.transform(test_pd['main_category'])\n",
    "\n",
    "# Hugging Face Dataset 轉換\n",
    "train_dataset = Dataset.from_pandas(train_pd[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_pd[['text', 'label']])\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Model 定義\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large\", num_labels=num_labels)\n",
    "\n",
    "# Trainer 參數設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # 改為 \"epoch\" 可以定期儲存模型\n",
    "    save_total_limit=1,     # 保留最近的一個 epoch 的模型 (可調整)\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 模型訓練\n",
    "trainer.train()\n",
    "\n",
    "# **儲存模型和 Tokenizer**\n",
    "output_model_path = \"./saved_model\"\n",
    "output_tokenizer_path = \"./saved_tokenizer\"\n",
    "\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_tokenizer_path)\n",
    "\n",
    "print(f\"模型已儲存至: {output_model_path}\")\n",
    "print(f\"Tokenizer 已儲存至: {output_tokenizer_path}\")\n",
    "\n",
    "# 儲存 LabelEncoder 的類別 (可選，但建議)\n",
    "import joblib\n",
    "output_label_encoder_path = \"./label_encoder.joblib\"\n",
    "joblib.dump(label_encoder, output_label_encoder_path)\n",
    "print(f\"LabelEncoder 已儲存至: {output_label_encoder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "617fa3d2-e90c-41f0-a2d1-17e23fc3ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2', '3': 'LABEL_3', '4': 'LABEL_4', '5': 'LABEL_5', '6': 'LABEL_6', '7': 'LABEL_7', '8': 'LABEL_8', '9': 'LABEL_9', '10': 'LABEL_10', '11': 'LABEL_11', '12': 'LABEL_12', '13': 'LABEL_13', '14': 'LABEL_14', '15': 'LABEL_15', '16': 'LABEL_16', '17': 'LABEL_17', '18': 'LABEL_18', '19': 'LABEL_19'}. The number of labels will be overwritten to 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.89      0.88      0.88        83\n",
      "    cond-mat       0.83      0.74      0.78       125\n",
      "          cs       0.94      0.89      0.91       624\n",
      "        econ       0.43      0.38      0.40         8\n",
      "        eess       0.52      0.68      0.59        63\n",
      "       gr-qc       0.48      0.68      0.57        19\n",
      "      hep-ex       0.67      0.75      0.71         8\n",
      "     hep-lat       0.00      0.00      0.00         8\n",
      "      hep-ph       0.74      0.85      0.79        34\n",
      "      hep-th       0.57      0.50      0.53        24\n",
      "        math       0.83      0.87      0.85       261\n",
      "     math-ph       0.00      0.00      0.00         7\n",
      "        nlin       0.00      0.00      0.00         6\n",
      "     nucl-ex       0.00      0.00      0.00         5\n",
      "     nucl-th       0.60      0.86      0.71         7\n",
      "     physics       0.67      0.67      0.67        90\n",
      "       q-bio       0.50      0.70      0.58        10\n",
      "       q-fin       0.50      0.60      0.55         5\n",
      "    quant-ph       0.71      0.86      0.78        73\n",
      "        stat       0.53      0.61      0.57        31\n",
      "\n",
      "    accuracy                           0.81      1491\n",
      "   macro avg       0.52      0.58      0.54      1491\n",
      "weighted avg       0.81      0.81      0.81      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 假設 train_preprocessed 和 test_preprocessed 是你的 Spark DataFrames\n",
    "# 這裡需要將 Spark DataFrames 轉換為 Pandas DataFrames\n",
    "train_pd = train_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "test_pd = test_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "\n",
    "# 設定模型、tokenizer 和 label encoder 的儲存路徑\n",
    "model_path = \"./saved_model\"\n",
    "tokenizer_path = \"./saved_tokenizer\"\n",
    "label_encoder_path = \"./label_encoder.joblib\"\n",
    "\n",
    "# 載入已訓練好的模型和 tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"mps\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# 載入儲存的 LabelEncoder\n",
    "loaded_label_encoder = joblib.load(label_encoder_path)\n",
    "label_names = loaded_label_encoder.classes_\n",
    "\n",
    "# 轉換為 Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_pd[['text', 'main_category']])\n",
    "\n",
    "# Tokenize 測試資料\n",
    "encoded_test = loaded_tokenizer(test_dataset['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 將模型移動到 GPU (如果可用)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoded_test = encoded_test.to(device)\n",
    "\n",
    "# 進行預測\n",
    "loaded_model.eval()\n",
    "\n",
    "# 創建 DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 將 encoded_test 轉換為 TensorDataset\n",
    "input_ids = encoded_test['input_ids']\n",
    "attention_mask = encoded_test['attention_mask']\n",
    "test_data = TensorDataset(input_ids, attention_mask)\n",
    "test_loader = DataLoader(test_data, batch_size=4)  # 調整 batch_size\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_attention_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    all_predictions.extend(predictions)\n",
    "\n",
    "y_pred_numeric = np.array(all_predictions)\n",
    "y_true_numeric = loaded_label_encoder.transform(test_pd['main_category'])\n",
    "\n",
    "# 將數值標籤轉換回原始字串標籤以便於理解報告\n",
    "y_pred_labels = loaded_label_encoder.inverse_transform(y_pred_numeric)\n",
    "y_true_labels = loaded_label_encoder.inverse_transform(y_true_numeric)\n",
    "\n",
    "# 顯示分類報告\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff378-cffc-49f4-bc45-51151b4ab2c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Clean text (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9c8f5-0a41-4133-b53e-4768f99059e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace newlines with spaces\n",
    "        text = text.replace('\\n', ' ')\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower() #lowercase\n",
    "    return text\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7282c-6a14-4e2a-9eaf-075d3978e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df['text'] = df['text'].apply(clean_text)\n",
    "df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ef716-4951-4be9-a0e4-d7513e135cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.preprocessing import LabelEncoder\n",
    "# 5. Encode categories\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['main_category'])\n",
    "\n",
    "# 6. Save cleaned data\n",
    "clean_df = df[['text', 'label', 'main_category']]\n",
    "clean_df.to_csv(\"cleaned_arxiv_data.csv\", index=False)\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(clean_df['main_category'].value_counts())\n",
    "# Label mapping\n",
    "print(\"\\nLabel mapping:\")\n",
    "for idx, cat in enumerate(le.classes_):\n",
    "    print(f\"{idx}: {cat}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5215a0-c744-4e61-acb9-c20fac3df6ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359f09-23a3-4877-94b7-4c73855d3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67f2b7-fa6b-480a-8b2d-e1a3e2aae33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2669e-ff4b-4445-8bfd-55c120962617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa4724-e825-434e-85f8-b2a25587cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42fa0a-455c-4346-a801-7e5e56142ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
