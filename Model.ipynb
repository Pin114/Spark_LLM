{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e4fe02-a841-4897-9c9d-fa6fc1159b37",
   "metadata": {},
   "source": [
    "# Loading in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02486e21-65c4-4024-91b8-8e34c82d8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenpinyu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd6112a-65b9-40ab-94fb-d7dec2ae91e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your collected CSV\n",
    "data_path = r\"/Users/chenpinyu/Desktop/spark/notebooks/Dirty_structured_output.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a44c601c-20b1-4a1e-a703-86810c1048d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aid</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>main_category</th>\n",
       "      <th>categories</th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://arxiv.org/abs/2504.02678v1</td>\n",
       "      <td>Valley and Spin Polarized States in Bernal Bil...</td>\n",
       "      <td>We present the results for the evolution of th...</td>\n",
       "      <td>cond-mat.str-el</td>\n",
       "      <td>cond-mat.str-el,cond-mat.mes-hall</td>\n",
       "      <td>2025-04-03T15:15:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://arxiv.org/abs/2504.15872v1</td>\n",
       "      <td>Multiscale detection of practically significan...</td>\n",
       "      <td>In many change point problems it is reasonable...</td>\n",
       "      <td>stat.ME</td>\n",
       "      <td>stat.ME,math.ST,stat.TH</td>\n",
       "      <td>2025-04-22T13:16:44Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://arxiv.org/abs/2504.17570v1</td>\n",
       "      <td>Atemporality from Conservation Laws of Physics...</td>\n",
       "      <td>Recent results have shown that singularities c...</td>\n",
       "      <td>gr-qc</td>\n",
       "      <td>gr-qc,astro-ph.HE,hep-th</td>\n",
       "      <td>2025-04-24T13:59:42Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://arxiv.org/abs/2504.16897v1</td>\n",
       "      <td>Assessing SSL/TLS Certificate Centralization: ...</td>\n",
       "      <td>SSL/TLS is a fundamental technology in the net...</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>cs.NI</td>\n",
       "      <td>2025-04-23T17:26:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://arxiv.org/abs/2504.11344v1</td>\n",
       "      <td>Interpretable Hybrid-Rule Temporal Point Proce...</td>\n",
       "      <td>Temporal Point Processes (TPPs) are widely use...</td>\n",
       "      <td>cs.LG</td>\n",
       "      <td>cs.LG,cs.AI,stat.ML</td>\n",
       "      <td>2025-04-15T16:15:16Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>http://arxiv.org/abs/2504.17321v1</td>\n",
       "      <td>Dargana: fine-tuning EarthPT for dynamic tree ...</td>\n",
       "      <td>We present Dargana, a fine-tuned variant of th...</td>\n",
       "      <td>physics.geo-ph</td>\n",
       "      <td>physics.geo-ph,cs.LG</td>\n",
       "      <td>2025-04-24T07:23:27Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>http://arxiv.org/abs/2504.01347v1</td>\n",
       "      <td>MEEK: Re-thinking Heterogeneous Parallel Error...</td>\n",
       "      <td>Heterogeneous parallel error detection is an a...</td>\n",
       "      <td>cs.AR</td>\n",
       "      <td>cs.AR</td>\n",
       "      <td>2025-04-02T04:32:49Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7699</th>\n",
       "      <td>http://arxiv.org/abs/2504.02741v1</td>\n",
       "      <td>A Complete Classification of Fourier Summation...</td>\n",
       "      <td>We completely classify Fourier summation formu...</td>\n",
       "      <td>math.CA</td>\n",
       "      <td>math.CA,math.MG,math.NT</td>\n",
       "      <td>2025-04-03T16:28:18Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>http://arxiv.org/abs/2503.24223v1</td>\n",
       "      <td>Jordanian deformation of the non-compact and $...</td>\n",
       "      <td>Using a Drinfeld twist of Jordanian type, we c...</td>\n",
       "      <td>hep-th</td>\n",
       "      <td>hep-th,cond-mat.stat-mech,cond-mat.str-el,quan...</td>\n",
       "      <td>2025-03-31T15:38:04Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7701</th>\n",
       "      <td>http://arxiv.org/abs/2504.10884v1</td>\n",
       "      <td>Unusual gas sensor response and semiconductor-...</td>\n",
       "      <td>WO3-x thinfilms featuring petal-like and lamel...</td>\n",
       "      <td>cond-mat.mtrl-sci</td>\n",
       "      <td>cond-mat.mtrl-sci,cond-mat.str-el</td>\n",
       "      <td>2025-04-15T05:29:29Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7702 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    aid  \\\n",
       "0     http://arxiv.org/abs/2504.02678v1   \n",
       "1     http://arxiv.org/abs/2504.15872v1   \n",
       "2     http://arxiv.org/abs/2504.17570v1   \n",
       "3     http://arxiv.org/abs/2504.16897v1   \n",
       "4     http://arxiv.org/abs/2504.11344v1   \n",
       "...                                 ...   \n",
       "7697  http://arxiv.org/abs/2504.17321v1   \n",
       "7698  http://arxiv.org/abs/2504.01347v1   \n",
       "7699  http://arxiv.org/abs/2504.02741v1   \n",
       "7700  http://arxiv.org/abs/2503.24223v1   \n",
       "7701  http://arxiv.org/abs/2504.10884v1   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Valley and Spin Polarized States in Bernal Bil...   \n",
       "1     Multiscale detection of practically significan...   \n",
       "2     Atemporality from Conservation Laws of Physics...   \n",
       "3     Assessing SSL/TLS Certificate Centralization: ...   \n",
       "4     Interpretable Hybrid-Rule Temporal Point Proce...   \n",
       "...                                                 ...   \n",
       "7697  Dargana: fine-tuning EarthPT for dynamic tree ...   \n",
       "7698  MEEK: Re-thinking Heterogeneous Parallel Error...   \n",
       "7699  A Complete Classification of Fourier Summation...   \n",
       "7700  Jordanian deformation of the non-compact and $...   \n",
       "7701  Unusual gas sensor response and semiconductor-...   \n",
       "\n",
       "                                                summary      main_category  \\\n",
       "0     We present the results for the evolution of th...    cond-mat.str-el   \n",
       "1     In many change point problems it is reasonable...            stat.ME   \n",
       "2     Recent results have shown that singularities c...              gr-qc   \n",
       "3     SSL/TLS is a fundamental technology in the net...              cs.NI   \n",
       "4     Temporal Point Processes (TPPs) are widely use...              cs.LG   \n",
       "...                                                 ...                ...   \n",
       "7697  We present Dargana, a fine-tuned variant of th...     physics.geo-ph   \n",
       "7698  Heterogeneous parallel error detection is an a...              cs.AR   \n",
       "7699  We completely classify Fourier summation formu...            math.CA   \n",
       "7700  Using a Drinfeld twist of Jordanian type, we c...             hep-th   \n",
       "7701  WO3-x thinfilms featuring petal-like and lamel...  cond-mat.mtrl-sci   \n",
       "\n",
       "                                             categories             published  \n",
       "0                     cond-mat.str-el,cond-mat.mes-hall  2025-04-03T15:15:42Z  \n",
       "1                               stat.ME,math.ST,stat.TH  2025-04-22T13:16:44Z  \n",
       "2                              gr-qc,astro-ph.HE,hep-th  2025-04-24T13:59:42Z  \n",
       "3                                                 cs.NI  2025-04-23T17:26:18Z  \n",
       "4                                   cs.LG,cs.AI,stat.ML  2025-04-15T16:15:16Z  \n",
       "...                                                 ...                   ...  \n",
       "7697                               physics.geo-ph,cs.LG  2025-04-24T07:23:27Z  \n",
       "7698                                              cs.AR  2025-04-02T04:32:49Z  \n",
       "7699                            math.CA,math.MG,math.NT  2025-04-03T16:28:18Z  \n",
       "7700  hep-th,cond-mat.stat-mech,cond-mat.str-el,quan...  2025-03-31T15:38:04Z  \n",
       "7701                  cond-mat.mtrl-sci,cond-mat.str-el  2025-04-15T05:29:29Z  \n",
       "\n",
       "[7702 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa70043-7dc7-4d01-9585-d4e8c0e6067b",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6fcd4a-5822-4355-8f72-b859f4a2db23",
   "metadata": {},
   "source": [
    "#### Dropping duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e12cbac-5cd5-49c3-b7ae-d14f579fc1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "## Drop duplicate rows from the dataset\n",
    "df=df.drop_duplicates()\n",
    "## Checking if some duplicates still remain based on \"aid\"(if the output is empty all duplicates have been removed)\n",
    "value_counts_result = df[\"aid\"].value_counts()\n",
    "filtered_result = value_counts_result[value_counts_result > 1]\n",
    "print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5164e4-209b-43de-b480-8dea5f46c555",
   "metadata": {},
   "source": [
    "#### Dropping rows with empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b21516-4298-44c6-b600-d2a90c3de3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['title'].notnull() & df['summary'].notnull()]\n",
    "df = df[df['categories'].notnull()]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dae779-81ee-4c00-aaea-711a1ea57418",
   "metadata": {},
   "source": [
    "#### Removing subcategories after \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd01a1fe-baf2-4b17-84b1-0bc154d8c8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['main_category'] = df['main_category'].apply(lambda x: x.split('.')[0] if pd.notnull(x) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343db9d7-0b0b-4c17-bbfa-1df3e69769e6",
   "metadata": {},
   "source": [
    "#### Combining Title and Summary into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fc127dd-57cb-4565-8ec0-011d0a36d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['title'] + ' ' + df['summary']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e591ca81-f613-43c7-8ae6-2a891dd36a26",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bd84abc-d4bb-4a9f-af98-c86a6d5b7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df[['text', 'main_category']], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db64083-7978-42a8-bcb7-4ec148f95eef",
   "metadata": {},
   "source": [
    "#### Convert to spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc5362-d499-40a1-849e-4d23e8ceae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 11:13:40 WARN Utils: Your hostname, Pins-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.46.219.58 instead (on interface en0)\n",
      "25/05/01 11:13:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/01 11:13:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CategoryPrediction\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"300s\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"1200\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .getOrCreate() #attempt by chatgpt to fix error i get in second cell below this\n",
    "\n",
    "train_df = spark.createDataFrame(train)\n",
    "test_df = spark.createDataFrame(test)\n",
    "train_df = train_df.repartition(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa54f5a-2eff-4b97-8abb-4551bc0d29fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "hashing_tf = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a3b68e-6145-4a58-bb47-913f0ca23160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "preprocessing_pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, hashing_tf, idf])\n",
    "preprocessor = preprocessing_pipeline.fit(train_df)\n",
    "\n",
    "train_preprocessed = preprocessor.transform(train_df)\n",
    "test_preprocessed = preprocessor.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d785bcd-3844-4e8c-82c9-946b1392acba",
   "metadata": {},
   "source": [
    "#### This was just to test whether tokenizer for example even works for a very small data example "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db4906a-e851-4c20-ae5c-b7dba80743cd",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381c966-be89-483d-821e-0629fb40dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# assume train_preprocessed & test_preprocessed are your Spark DataFrames\n",
    "spark = SparkSession.builder.appName(\"CategoryPrediction\").getOrCreate()\n",
    "train_pd = train_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "test_pd = test_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "\n",
    "# encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_pd['label'] = label_encoder.fit_transform(train_pd['main_category'])\n",
    "test_pd['label'] = label_encoder.transform(test_pd['main_category'])\n",
    "\n",
    "# Hugging Face Dataset transformation\n",
    "train_dataset = Dataset.from_pandas(train_pd[['text', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_pd[['text', 'label']])\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Model definition\n",
    "num_labels = len(label_encoder.classes_)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/bart-large\", num_labels=num_labels)\n",
    "\n",
    "# Trainer parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    #evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  \n",
    "    save_total_limit=1,     # keep the model of the latest epoch\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# model training\n",
    "trainer.train()\n",
    "\n",
    "# **Model saving和 Tokenizer**\n",
    "output_model_path = \"./saved_model\"\n",
    "output_tokenizer_path = \"./saved_tokenizer\"\n",
    "\n",
    "model.save_pretrained(output_model_path)\n",
    "tokenizer.save_pretrained(output_tokenizer_path)\n",
    "\n",
    "print(f\"Model Saved to: {output_model_path}\")\n",
    "print(f\"Tokenizer Saved to: {output_tokenizer_path}\")\n",
    "\n",
    "\n",
    "import joblib\n",
    "output_label_encoder_path = \"./label_encoder.joblib\"\n",
    "joblib.dump(label_encoder, output_label_encoder_path)\n",
    "print(f\"LabelEncoder Saved to: {output_label_encoder_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "617fa3d2-e90c-41f0-a2d1-17e23fc3ace1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'LABEL_0', '1': 'LABEL_1', '2': 'LABEL_2', '3': 'LABEL_3', '4': 'LABEL_4', '5': 'LABEL_5', '6': 'LABEL_6', '7': 'LABEL_7', '8': 'LABEL_8', '9': 'LABEL_9', '10': 'LABEL_10', '11': 'LABEL_11', '12': 'LABEL_12', '13': 'LABEL_13', '14': 'LABEL_14', '15': 'LABEL_15', '16': 'LABEL_16', '17': 'LABEL_17', '18': 'LABEL_18', '19': 'LABEL_19'}. The number of labels will be overwritten to 20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    astro-ph       0.89      0.88      0.88        83\n",
      "    cond-mat       0.83      0.74      0.78       125\n",
      "          cs       0.94      0.89      0.91       624\n",
      "        econ       0.43      0.38      0.40         8\n",
      "        eess       0.52      0.68      0.59        63\n",
      "       gr-qc       0.48      0.68      0.57        19\n",
      "      hep-ex       0.67      0.75      0.71         8\n",
      "     hep-lat       0.00      0.00      0.00         8\n",
      "      hep-ph       0.74      0.85      0.79        34\n",
      "      hep-th       0.57      0.50      0.53        24\n",
      "        math       0.83      0.87      0.85       261\n",
      "     math-ph       0.00      0.00      0.00         7\n",
      "        nlin       0.00      0.00      0.00         6\n",
      "     nucl-ex       0.00      0.00      0.00         5\n",
      "     nucl-th       0.60      0.86      0.71         7\n",
      "     physics       0.67      0.67      0.67        90\n",
      "       q-bio       0.50      0.70      0.58        10\n",
      "       q-fin       0.50      0.60      0.55         5\n",
      "    quant-ph       0.71      0.86      0.78        73\n",
      "        stat       0.53      0.61      0.57        31\n",
      "\n",
      "    accuracy                           0.81      1491\n",
      "   macro avg       0.52      0.58      0.54      1491\n",
      "weighted avg       0.81      0.81      0.81      1491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/chenpinyu/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Spark DataFrames -> Pandas DataFrames\n",
    "train_pd = train_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "test_pd = test_preprocessed.select(\"text\", \"main_category\").toPandas()\n",
    "\n",
    "model_path = \"./saved_model\"\n",
    "tokenizer_path = \"./saved_tokenizer\"\n",
    "label_encoder_path = \"./label_encoder.joblib\"\n",
    "\n",
    "# load model and tokenizer\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"mps\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# load LabelEncoder\n",
    "loaded_label_encoder = joblib.load(label_encoder_path)\n",
    "label_names = loaded_label_encoder.classes_\n",
    "\n",
    "# transform to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_pd[['text', 'main_category']])\n",
    "\n",
    "# Tokenize test data\n",
    "encoded_test = loaded_tokenizer(test_dataset['text'], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# transfer the model to GPU( if applicable)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoded_test = encoded_test.to(device)\n",
    "\n",
    "# predicting\n",
    "loaded_model.eval()\n",
    "\n",
    "# create DataLoader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#  encoded_test -> TensorDataset\n",
    "input_ids = encoded_test['input_ids']\n",
    "attention_mask = encoded_test['attention_mask']\n",
    "test_data = TensorDataset(input_ids, attention_mask)\n",
    "test_loader = DataLoader(test_data, batch_size=4)  # adjust batch_size\n",
    "\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for batch in test_loader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_attention_mask = batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids=b_input_ids, attention_mask=b_attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "    all_predictions.extend(predictions)\n",
    "\n",
    "y_pred_numeric = np.array(all_predictions)\n",
    "y_true_numeric = loaded_label_encoder.transform(test_pd['main_category'])\n",
    "\n",
    "# index label -> String\n",
    "y_pred_labels = loaded_label_encoder.inverse_transform(y_pred_numeric)\n",
    "y_true_labels = loaded_label_encoder.inverse_transform(y_true_numeric)\n",
    "\n",
    "print(classification_report(y_true_labels, y_pred_labels, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff378-cffc-49f4-bc45-51151b4ab2c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Clean text (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c9c8f5-0a41-4133-b53e-4768f99059e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace newlines with spaces\n",
    "        text = text.replace('\\n', ' ')\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        return text.lower() #lowercase\n",
    "    return text\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7282c-6a14-4e2a-9eaf-075d3978e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df['text'] = df['text'].apply(clean_text)\n",
    "df\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ef716-4951-4be9-a0e4-d7513e135cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.preprocessing import LabelEncoder\n",
    "# 5. Encode categories\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['main_category'])\n",
    "\n",
    "# 6. Save cleaned data\n",
    "clean_df = df[['text', 'label', 'main_category']]\n",
    "clean_df.to_csv(\"cleaned_arxiv_data.csv\", index=False)\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(clean_df['main_category'].value_counts())\n",
    "# Label mapping\n",
    "print(\"\\nLabel mapping:\")\n",
    "for idx, cat in enumerate(le.classes_):\n",
    "    print(f\"{idx}: {cat}\")\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5215a0-c744-4e61-acb9-c20fac3df6ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359f09-23a3-4877-94b7-4c73855d3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"facebook/bart-large-mnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a67f2b7-fa6b-480a-8b2d-e1a3e2aae33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2669e-ff4b-4445-8bfd-55c120962617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa4724-e825-434e-85f8-b2a25587cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba42fa0a-455c-4346-a801-7e5e56142ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
